{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b30243",
   "metadata": {},
   "source": [
    "# Realized Volatility Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ef4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ecc19",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"1990-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "\n",
    "spyTicker = yf.Ticker(\"SPY\") # SPDR S&P 500 ETF Trust\n",
    "vixTicker = yf.Ticker(\"^VIX\") # VIX\n",
    "df_spy = spyTicker.history(start=START_DATE, end=END_DATE)\n",
    "df_vix = vixTicker.history(start=START_DATE, end=END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra rows from vix to match spy\n",
    "df_vix = df_vix.drop(df_vix.loc['1990':'1993-01-28'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd36caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Timestamps to make rows line up better across datasets\n",
    "df_spy.index = df_spy.index.date\n",
    "df_vix.index = df_vix.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ef816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join datasets\n",
    "dataset = pd.merge(df_spy, df_vix, left_index=True, right_index=True, suffixes=('_SPY','_VIX'))\n",
    "dataset = dataset.drop(['Dividends_SPY','Stock Splits_SPY', 'Capital Gains', 'Dividends_VIX', 'Stock Splits_VIX', 'Volume_VIX'], axis=1)\n",
    "\n",
    "# Load FFER data, make Column A the index\n",
    "FFER = pd.read_csv(\"FFER.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Rename FFER column \n",
    "FFER.columns = [\"FFER\"]\n",
    "\n",
    "# Merge on the index\n",
    "dataset = dataset.join(FFER, how=\"left\")\n",
    "\n",
    "# Load US Economic Uncertainty data, make Column A the index\n",
    "econ_uncert = pd.read_csv(\"FFER.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Rename US Econmic Unceratainty column \n",
    "econ_uncert.columns = [\"Economic Uncertainty\"]\n",
    "\n",
    "# Merge on the index\n",
    "dataset = dataset.join(econ_uncert, how=\"left\")\n",
    "\n",
    "print(dataset.head())\n",
    "print(dataset.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 3M T-Bill yield\n",
    "tbill = yf.Ticker(\"^IRX\")\n",
    "df_tbill = tbill.history(period=\"max\")\n",
    "\n",
    "# Align date index\n",
    "df_tbill.index = df_tbill.index.date\n",
    "df_tbill = df_tbill[['Close']] # Take yield\n",
    "\n",
    "# Rename column\n",
    "df_tbill.columns = ['TBill_3M']\n",
    "\n",
    "# Merge with your main dataset\n",
    "dataset = dataset.join(df_tbill, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61445957",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4048b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPY features\n",
    "# daily log price and  natural log-returns for SPY\n",
    "dataset[\"log_close_SPY\"] = np.log(dataset[\"Close_SPY\"])\n",
    "dataset[\"log_open_SPY\"] = np.log(dataset[\"Open_SPY\"])\n",
    "dataset[\"ret_1d\"] = dataset[\"log_close_SPY\"].diff(1)   # daily natural log-return\n",
    "\n",
    "# intraday turbulance\n",
    "dataset[\"intraday_OC_abs\"] = np.abs(dataset[\"log_open_SPY\"] - dataset[\"log_close_SPY\"])\n",
    "dataset[\"intraday_OC\"] = dataset[\"log_open_SPY\"] - dataset[\"log_close_SPY\"]\n",
    "dataset[\"intraday_CO_abs\"] = np.abs(dataset[\"log_close_SPY\"].shift(1) - dataset[\"log_open_SPY\"]) #What happend overnight? price jumped/Crashed? \n",
    "dataset[\"intraday_CH\"] = dataset[\"log_close_SPY\"].shift(1) - np.log(dataset[\"High_SPY\"])\n",
    "dataset[\"intraday_CL\"] = dataset[\"log_close_SPY\"].shift(1) - np.log(dataset[\"Low_SPY\"])\n",
    "dataset[\"intraday_CO\"] = dataset[\"log_close_SPY\"].shift(1) - dataset[\"log_open_SPY\"]\n",
    "dataset[\"intraday_HL\"] = np.log(dataset[\"High_SPY\"]) - np.log(dataset[\"Low_SPY\"])\n",
    "dataset[\"intraday_HL_abs\"] = np.abs(np.log(dataset[\"High_SPY\"]) - np.log(dataset[\"Low_SPY\"]))\n",
    "\n",
    "# lagged returns (1 and 2 days)\n",
    "dataset[\"ret_lag1\"] = dataset[\"ret_1d\"].shift(1)\n",
    "dataset[\"ret_lag2\"] = dataset[\"ret_1d\"].shift(2)\n",
    "dataset[\"intraday_HL_abs_lag1\"] = dataset[\"intraday_HL_abs\"].shift(1)\n",
    "dataset[\"intraday_HL_abs_lag2\"] = dataset[\"intraday_HL_abs\"].shift(2)\n",
    "dataset[\"intraday_HL_lag1\"] = dataset[\"intraday_HL\"].shift(1)\n",
    "dataset[\"intraday_HL_lag2\"] = dataset[\"intraday_HL\"].shift(2)\n",
    "\n",
    "# rolling statistics of returns\n",
    "dataset[\"ret_roll_mean_5d\"] = dataset[\"ret_1d\"].rolling(5, min_periods=1).mean()\n",
    "dataset[\"ret_roll_std_5d\"] = dataset[\"ret_1d\"].rolling(5, min_periods=1).std()\n",
    "dataset[\"ret_roll_abs_5d\"] = np.abs(dataset[\"ret_1d\"].rolling(5, min_periods=1).sum())\n",
    "\n",
    "dataset[\"ret_roll_mean_22d\"] = dataset[\"ret_1d\"].rolling(22, min_periods=1).mean()\n",
    "dataset[\"ret_roll_std_22d\"] = dataset[\"ret_1d\"].rolling(22, min_periods=1).std()\n",
    "dataset[\"ret_roll_abs_22d\"] = np.abs(dataset[\"ret_1d\"].rolling(22, min_periods=1).sum())\n",
    "\n",
    "dataset[\"intraday_roll_CO_mean_5d\"] = dataset[\"intraday_CO\"].rolling(5, min_periods=1).mean()\n",
    "dataset[\"intraday_roll_CO_std_5d\"] = dataset[\"intraday_CO\"].rolling(5,min_periods=1).std()\n",
    "dataset[\"intraday_roll_CO_mean_22d\"] = dataset[\"intraday_CO\"].rolling(22,min_periods=1).mean()\n",
    "dataset[\"intraday_roll_CO_std_22d\"] = dataset[\"intraday_CO\"].rolling(22,min_periods=1).std()\n",
    "\n",
    "\n",
    "# Technical indicators: moving averages on Close (5, 10, 22 days)\n",
    "dataset[\"ma_5\"] = dataset[\"log_close_SPY\"].rolling(window=5, min_periods=1).mean()\n",
    "dataset[\"ma_10\"] = dataset[\"log_close_SPY\"].rolling(window=10,min_periods=1).mean()\n",
    "dataset[\"ma_22\"] = dataset[\"log_close_SPY\"].rolling(window=22,min_periods=1).mean()\n",
    "\n",
    "dataset[\"momentum_5d\"] = dataset[\"log_close_SPY\"] - dataset[\"log_close_SPY\"].shift(5)\n",
    "dataset[\"momentum_22d\"] = dataset[\"log_close_SPY\"] - dataset[\"log_close_SPY\"].shift(22)\n",
    "\n",
    "#Relative Strength Index. Measure between 0 - 100 indicating if more gains or loss has occured in predefined time period\n",
    "delta = dataset[\"ret_1d\"]\n",
    "\n",
    "gain = delta.clip(lower=0)  \n",
    "loss = -delta.clip(upper=0) \n",
    "\n",
    "avg_gain = gain.rolling(5, min_periods=1).mean()\n",
    "avg_loss = loss.rolling(5, min_periods=1).mean()\n",
    "\n",
    "# Avoid divide-by-zero\n",
    "avg_loss = avg_loss.replace(0, 1e-10)\n",
    "\n",
    "RS = avg_gain / avg_loss\n",
    "RSI = 100 - (100 / (1 + RS))\n",
    "\n",
    "dataset[\"RSI_5d\"] = RSI\n",
    "\n",
    "# Bollinger Bands. Measure how close or far is the current price from the moving average of a predefine time period\n",
    "dataset[\"BB_low\"] = dataset[\"ma_22\"] - (2 * dataset[\"log_close_SPY\"].rolling(22).std())\n",
    "dataset[\"BB_high\"] = dataset[\"ma_22\"] + (2 * dataset[\"log_close_SPY\"].rolling(22).std())\n",
    "\n",
    "dataset[\"BB_percentage_diff\"] =  (dataset[\"BB_high\"] - dataset[\"BB_low\"]) / dataset[\"ma_22\"]\n",
    "dataset[\"BB_price_location\"] =  (dataset[\"log_close_SPY\"] - dataset[\"BB_low\"]) / (dataset[\"BB_high\"] - dataset[\"BB_low\"] + 1e-10)\n",
    "\n",
    "#Average True Range. Pick the largets movement between H/L, Ct-1/H, Ct-1/L\n",
    "dataset[\"TR\"] = dataset[[\"intraday_HL\",\"intraday_CH\", \"intraday_CL\"]].max(axis=1)\n",
    "dataset[\"ATR\"] = dataset[\"TR\"].rolling(14, min_periods=1).mean()\n",
    "\n",
    "# Rate of change (ROC) – 5 and 10 day\n",
    "dataset[\"roc_5\"] = dataset[\"Close_SPY\"].pct_change(periods=5)\n",
    "dataset[\"roc_10\"] = dataset[\"Close_SPY\"].pct_change(periods=10)\n",
    "\n",
    "# Volume-related feature (log-volume + 10-day rolling mean)\n",
    "dataset[\"log_volume\"] = np.log(dataset[\"Volume_SPY\"].replace(0, np.nan))\n",
    "dataset[\"vol_roll_mean_10\"] = dataset[\"log_volume\"].rolling(window=10,min_periods=1).mean()\n",
    "\n",
    "# Realized variance and lagged relalized variance daily, weekly, monthly\n",
    "dataset[\"rvar_1d\"] = dataset[\"ret_1d\"] ** 2\n",
    "dataset[\"rvar_5d\"] = dataset[\"rvar_1d\"].rolling(window=5,min_periods=1).sum()\n",
    "dataset[\"rvar_22d\"] = dataset[\"rvar_1d\"].rolling(window=22,min_periods=1).sum()\n",
    "\n",
    "# Realized volatility and lagged relalized volatility daily, weekly, monthly\n",
    "dataset[\"rvol_1d\"] = np.sqrt(dataset[\"rvar_1d\"]) # Y lable\n",
    "dataset[\"rvol_5d\"] = np.sqrt(dataset[\"rvar_5d\"])\n",
    "dataset[\"rvol_22d\"] = np.sqrt(dataset[\"rvar_22d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4dedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da015c2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f531e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "y = dataset[\"rvol_1d\"].shift(-1)\n",
    "\n",
    "print(\"shape of x:\", X.shape)\n",
    "print(\"shape of y:\", y.tail())\n",
    "X = X.iloc[:-1]\n",
    "y = y.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of x:\", X.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "print(\"last 5 rows of y:\", y.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641df371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data between train, validate, test (60, 20, 20)\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31840df",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000, \n",
    "    max_depth=8,    \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf.predict(X_train)\n",
    "y_test_pred_rf  = rf.predict(X_test)\n",
    "\n",
    "# Training metrics \n",
    "train_mse_rf  = mean_squared_error(y_train, y_train_pred_rf)\n",
    "train_rmse_rf = train_mse_rf ** 0.5\n",
    "train_mae_rf  = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "train_r2_rf   = r2_score(y_train, y_train_pred_rf)\n",
    "\n",
    "# Testing metrics\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_rmse_rf = test_mse_rf**0.5\n",
    "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "test_r2_rf = rf.score(X_test, y_test)\n",
    "\n",
    "# Print comparison \n",
    "print(\"Training Metrics\")\n",
    "print(\"RMSE (train):\", train_rmse_rf)\n",
    "print(\"MAE  (train):\", train_mae_rf)\n",
    "print(\"R^2  (train):\", train_r2_rf)\n",
    "\n",
    "print(\"\\nTest Metrics\")\n",
    "print(\"RMSE:\", test_rmse_rf)\n",
    "print(\"MAE:\", test_mae_rf)\n",
    "print(\"R^2: \", test_r2_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4ca80",
   "metadata": {},
   "source": [
    "## Most Representative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": rf.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3225361",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac94447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data with 0 mean and variance\n",
    "std = StandardScaler().fit(X_train)\n",
    "X_train_std = std.transform(X_train)\n",
    "X_val_std = std.transform(X_val)\n",
    "X_test_std = std.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr.predict(X_train_std)\n",
    "y_test_pred_lr  = lr.predict(X_test_std)\n",
    "\n",
    "# Training metrics \n",
    "train_mse_lr  = mean_squared_error(y_train, y_train_pred_lr)\n",
    "train_rmse_lr = train_mse_lr ** 0.5\n",
    "train_mae_lr  = mean_absolute_error(y_train, y_train_pred_lr)\n",
    "train_r2_lr   = r2_score(y_train, y_train_pred_lr)\n",
    "\n",
    "# Test metrics\n",
    "test_mse_lr  = mean_squared_error(y_test, y_test_pred_lr)\n",
    "test_rmse_lr = test_mse_lr ** 0.5\n",
    "test_mae_lr  = mean_absolute_error(y_test, y_test_pred_lr)\n",
    "test_r2_lr  = r2_score(y_test, y_test_pred_lr)\n",
    "\n",
    "# Print comparison \n",
    "print(\"Training Metrics\")\n",
    "print(\"RMSE (train):\", train_rmse_lr)\n",
    "print(\"MAE  (train):\", train_mae_lr)\n",
    "print(\"R^2  (train):\", train_r2_lr)\n",
    "\n",
    "print(\"\\nTest Metrics\")\n",
    "print(\"RMSE (test):\", test_rmse_lr)\n",
    "print(\"MAE  (test):\", test_mae_lr)\n",
    "print(\"R^2  (test):\", test_r2_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d2b58",
   "metadata": {},
   "source": [
    "## Most Representative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": lr.coef_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6cacf",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c453fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(\n",
    "    n_estimators=2500,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=2,\n",
    "    min_child_weight=5,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_xgb = xgb.predict(X_train)\n",
    "y_test_pred_xgb  = xgb.predict(X_test)\n",
    "\n",
    "# Training metrics \n",
    "train_mse_xgb  = mean_squared_error(y_train, y_train_pred_xgb)\n",
    "train_rmse_xgb = train_mse_xgb ** 0.5\n",
    "train_mae_xgb  = mean_absolute_error(y_train, y_train_pred_xgb)\n",
    "train_r2_xgb   = r2_score(y_train, y_train_pred_xgb)\n",
    "\n",
    "# Test metrics\n",
    "test_mse_xgb  = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "test_rmse_xgb = test_mse_xgb ** 0.5\n",
    "test_mae_xgb  = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "test_r2_xgb   = r2_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "# Print comparison \n",
    "print(\"Training Metrics\")\n",
    "print(\"RMSE (train):\", train_rmse_xgb)\n",
    "print(\"MAE  (train):\", train_mae_xgb)\n",
    "print(\"R^2  (train):\", train_r2_xgb)\n",
    "\n",
    "print(\"\\nTest Metrics\")\n",
    "print(\"RMSE (test):\", test_rmse_xgb)\n",
    "print(\"MAE  (test):\", test_mae_xgb)\n",
    "print(\"R^2  (test):\", test_r2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e298f3",
   "metadata": {},
   "source": [
    "## Most Representative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72761167",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": xgb.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d76532",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26010bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_lstm = [\n",
    "    \"ret_1d\", \"intraday_OC\", \"intraday_CO\", \"intraday_HL\",\n",
    "    \"rvol_5d\", \"rvol_22d\",              # keep past vol, drop rvol_1d from features\n",
    "    \"Close_VIX\", \"High_VIX\", \"Low_VIX\",\n",
    "    \"FFER\", \"TBill_3M\"\n",
    "]\n",
    "\n",
    "label_col = \"rvol_1d\"\n",
    "\n",
    "data_lstm = dataset[feature_cols_lstm + [label_col]].dropna().copy()\n",
    "\n",
    "# next-day label\n",
    "data_lstm[\"rv_next\"] = data_lstm[label_col].shift(-1)\n",
    "data_lstm = data_lstm.dropna()\n",
    "\n",
    "data_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfa37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data_lstm)\n",
    "train_end = int(0.6 * n)\n",
    "val_end   = int(0.8 * n)\n",
    "\n",
    "train_df = data_lstm.iloc[:train_end]\n",
    "val_df   = data_lstm.iloc[train_end:val_end]\n",
    "test_df  = data_lstm.iloc[val_end:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[feature_cols_lstm])\n",
    "\n",
    "train_feats = scaler.transform(train_df[feature_cols_lstm])\n",
    "val_feats   = scaler.transform(val_df[feature_cols_lstm])\n",
    "test_feats  = scaler.transform(test_df[feature_cols_lstm])\n",
    "\n",
    "train_targets = train_df[\"rv_next\"].values\n",
    "val_targets   = val_df[\"rv_next\"].values\n",
    "test_targets  = test_df[\"rv_next\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8982ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "\n",
    "def make_sequences(features_2d, targets_1d, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features_2d) - seq_len):\n",
    "        X.append(features_2d[i:i+seq_len])\n",
    "        y.append(targets_1d[i+seq_len-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train_lstm, y_train_lstm = make_sequences(train_feats, train_targets, seq_len)\n",
    "X_val_lstm,   y_val_lstm   = make_sequences(val_feats, val_targets, seq_len)\n",
    "X_test_lstm,  y_test_lstm  = make_sequences(test_feats, test_targets, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(VolDataset(X_train_lstm, y_train_lstm), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(VolDataset(X_val_lstm,   y_val_lstm),   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(VolDataset(X_test_lstm,  y_test_lstm),  batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(feature_cols_lstm)\n",
    "hidden_dim = 64\n",
    "\n",
    "class LSTMVolPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        h_last = lstm_out[:, -1, :] # last hidden state\n",
    "        x = self.fc1(h_last)\n",
    "        x = self.act(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = LSTMVolPredictor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf769ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(Xb).squeeze()\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(Xb)\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            preds = model(Xb).squeeze()\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item() * len(Xb)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Train MSE={train_loss:.6f}  Val MSE={val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, _ in test_loader:\n",
    "        preds = model(Xb).squeeze()\n",
    "        all_preds.extend(preds.numpy())\n",
    "\n",
    "y_test_pred_lstm = np.array(all_preds)\n",
    "\n",
    "test_mse_lstm  = mean_squared_error(y_test_lstm, y_test_pred_lstm)\n",
    "test_rmse_lstm = np.sqrt(test_mse_lstm)\n",
    "test_mae_lstm  = mean_absolute_error(y_test_lstm, y_test_pred_lstm)\n",
    "test_r2_lstm   = r2_score(y_test_lstm, y_test_pred_lstm)\n",
    "\n",
    "print(f\"LSTM Test RMSE: {test_rmse_lstm:.6f}\")\n",
    "print(f\"LSTM Test MAE : {test_mae_lstm:.6f}\")\n",
    "print(f\"LSTM Test R²  : {test_r2_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b147a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(model, X):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), 64):  # batch size 64\n",
    "            Xb = torch.tensor(X[i:i+64], dtype=torch.float32)\n",
    "            out = model(Xb).cpu().numpy().flatten()\n",
    "            preds.extend(out)\n",
    "    return np.array(preds)\n",
    "\n",
    "baseline_pred = rnn_predict(model, X_test_lstm)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test_lstm, baseline_pred))\n",
    "baseline_rmse\n",
    "\n",
    "feature_names = feature_cols_lstm # same order as input_dim\n",
    "\n",
    "importances = []\n",
    "\n",
    "for f in range(X_test_lstm.shape[2]): # loop over features\n",
    "    X_test_permuted = X_test_lstm.copy()\n",
    "    np.random.shuffle(X_test_permuted[:, :, f]) # shuffle this feature across sequences\n",
    "    \n",
    "    perm_pred = rnn_predict(model, X_test_permuted)\n",
    "    perm_rmse = np.sqrt(mean_squared_error(y_test_lstm, perm_pred))\n",
    "    \n",
    "    importance = perm_rmse - baseline_rmse # increase in error\n",
    "    importances.append(importance)\n",
    "\n",
    "fi_rnn = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "fi_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4cb98",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_names = ['RMSE', 'MAE', 'R^2']\n",
    "index_names = ['Random Forest', 'Linear Regression', 'XGB', 'LSTM']\n",
    "\n",
    "metrics = pd.DataFrame(index=index_names, columns=cols_names)\n",
    "metrics.loc['Random Forest'] = [test_rmse_rf, test_mae_rf, test_r2_rf]\n",
    "metrics.loc['Linear Regression'] = [test_rmse_lr, test_mae_lr, test_r2_lr]\n",
    "metrics.loc['XGB'] = [test_rmse_xgb, test_mae_xgb, test_r2_xgb]\n",
    "metrics.loc['LSTM'] = [test_rmse_lstm, test_mae_lstm, test_r2_lstm]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96df9f9",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = {\n",
    "    \"actual\": \"#2200FF\",  # blue\n",
    "    \"pred\":   \"#E63946\",  # red\n",
    "}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14,5))\n",
    "\n",
    "# plot realized volatility\n",
    "line1, = ax1.plot(\n",
    "    dataset.index, \n",
    "    dataset[\"rvol_1d\"], \n",
    "    color=COLORS[\"actual\"], \n",
    "    label=\"Realized Volatility\",\n",
    "    linewidth=0.4\n",
    ")\n",
    "ax1.set_ylabel(\"Realized Volatility\", color=COLORS[\"actual\"])\n",
    "ax1.set_xlabel(\"Date\")\n",
    "\n",
    "# twin axis for SPY\n",
    "ax2 = ax1.twinx()\n",
    "line2, = ax2.plot(\n",
    "    dataset.index, \n",
    "    dataset[\"Close_SPY\"], \n",
    "    color=COLORS[\"pred\"], \n",
    "    label=\"SPY Close\",\n",
    "    linewidth=0.8\n",
    ")\n",
    "ax2.set_ylabel(\"SPY Price\", color=\"red\")\n",
    "ax2.set_xlabel(\"Date\")\n",
    "\n",
    "# combine legends\n",
    "lines = [line1, line2]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc=\"upper left\")\n",
    "\n",
    "plt.title(\"SPY Price vs Realized Volatility\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = X_test.index  # dates\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Actual RV\": y_test,\n",
    "    \"RF Prediction\": y_test_pred_rf,\n",
    "    \"LR Prediction\": y_test_pred_lr,\n",
    "    \"XGB Prediction\": y_test_pred_xgb\n",
    "}, index=test_index)\n",
    "\n",
    "n_lstm = len(y_test_pred_lstm)\n",
    "\n",
    "# align LSTM predictions to the *last* n_lstm dates of the test period\n",
    "lstm_index = test_index[-n_lstm:]\n",
    "len(lstm_index), len(y_test_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(results.index, results[\"Actual RV\"], color=COLORS[\"actual\"], label=\"Actual\", linewidth=1)\n",
    "plt.plot(results.index, results[\"RF Prediction\"], color=COLORS[\"pred\"], label=\"RF Pred\", linewidth=1)\n",
    "plt.title(\"Random Forest – Actual vs Predicted Volatility (Test Set)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(results.index, results[\"Actual RV\"], color=COLORS[\"actual\"], label=\"Actual\", linewidth=1)\n",
    "plt.plot(results.index, results[\"LR Prediction\"], color=COLORS[\"pred\"], label=\"LR Pred\", linewidth=1)\n",
    "plt.title(\"Linear Regression – Actual vs Predicted Volatility (Test Set)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf503f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(results.index, results[\"Actual RV\"], color=COLORS[\"actual\"], label=\"Actual\", linewidth=1)\n",
    "plt.plot(results.index, results[\"XGB Prediction\"], color=COLORS[\"pred\"], label=\"XGB Pred\", linewidth=1)\n",
    "plt.title(\"XGBoost – Actual vs Predicted Volatility (Test Set)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(test_index, y_test, color=COLORS[\"actual\"], label=\"Actual RV\", linewidth=1.5)\n",
    "plt.plot(lstm_index, y_test_pred_lstm, color=COLORS[\"pred\"], label=\"LSTM Pred\", linewidth=1)\n",
    "plt.title(\"Actual vs LSTM-Predicted Volatility (Test Set)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Realized Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
